{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "#import data\n",
    "df=pd.read_csv('dataProject.csv', header=0); \n",
    "\n",
    "X1=df.ix[:,2:154] #instances\n",
    "#X1.head()\n",
    "X=X1.values\n",
    "\n",
    "# standardscale data\n",
    "from sklearn import preprocessing\n",
    "X_scaled=preprocessing.scale(X)\n",
    "\n",
    "pca = PCA(n_components=2).fit(X_scaled)\n",
    "X_2d = pca.transform(X_scaled)\n",
    "\n",
    "#define lables\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = df.loc[:, 'Label'].values\n",
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set training set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_2d, y, test_size=0.20, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.65467555  0.23347428]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 99.51068333  35.48809009]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32694666509279369"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.noise_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#=====================================================================================================\n",
    "#Fig 3\n",
    "#=====================================================================================================\n",
    "pl.close('all')\n",
    "pl.figure(figsize=(8,6))\n",
    "h = .02  # step size in the mesh\n",
    "#C = 10  # SVM regularization parameter\n",
    "svc = svm.SVC(kernel='linear').fit(X_train, y_train)\n",
    "rbf_svc = svm.SVC(kernel='rbf').fit(X_train, y_train)\n",
    "poly_svc = svm.SVC(kernel='poly').fit(X_train, y_train)\n",
    "sigmod_svc = svm.SVC(kernel='sigmoid').fit(X_train, y_train)\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "# title for the plots\n",
    "titles = ['SVC with linear kernel',\n",
    "          'SVC with sigmoid kernel',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel']\n",
    "\n",
    "\n",
    "for k, clf in enumerate((svc, sigmod_svc, rbf_svc, poly_svc)):\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    pl.subplot(2, 2, k + 1)\n",
    "    pl.subplots_adjust(wspace=0.08, hspace=0.20)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    pl.contourf(xx, yy, Z, cmap=pl.cm.coolwarm, alpha=0.5)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pl.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap=pl.cm.coolwarm,s=20) \n",
    "\n",
    "    #Plot score\n",
    "    \n",
    "    pl.text(0.6,0.9, ('Test accuracy: %.2f' % clf.score(X_2d, y)), transform=pl.gca().transAxes,\n",
    "            bbox={'facecolor':'white'})\n",
    "\n",
    "\n",
    "    pl.xlabel('Principal component 1')\n",
    "    pl.ylabel('Principal component 2')\n",
    "    pl.xlim(xx.min(), xx.max())\n",
    "    pl.ylim(yy.min(), yy.max())\n",
    "    pl.xticks(())\n",
    "    pl.yticks(())\n",
    "    pl.title(titles[k])\n",
    "\n",
    "pl.subplots_adjust(left=0.03, right=0.99, bottom=0.04, top=0.95)    \n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [ 1.    1.    1.    1.    1.    1.    1.    0.75  1.    1.  ]\n",
      "CV accuracy: 0.975 +/- 0.075\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_2d, y, test_size=0.20, random_state=20)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores = cross_val_score(estimator=svm.SVC(kernel='rbf', gamma=0.01, C=10, random_state=69), \n",
    "                         X=X_train, \n",
    "                         y=y_train, \n",
    "                         cv=KFold(n_splits=10,random_state=1),\n",
    "                         n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)\n",
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "The best parameters are {'C': 10.0, 'gamma': 0.01} with a score of 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib \n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_2d, y, test_size=0.20, random_state=20)\n",
    "\n",
    "C_range = np.logspace(-5, 7, 13)\n",
    "gamma_range = np.logspace(-8, 4, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=3)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"The best parameters are %s with a score of %0.3f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "\n",
    "# Now we need to fit a classifier for all parameters in the 2d version\n",
    "# (we use a smaller set of parameters here because it takes a while to train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fig 5\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "#\n",
    "# The score are encoded as colors with the hot colormap which varies from dark\n",
    "# red to bright yellow. As the most interesting scores are all located in the\n",
    "# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n",
    "# as to make it easier to visualize the small variations of score values in the\n",
    "# interesting range while not brutally collapsing all the low score values to\n",
    "# the same color.\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "        train_test_split(X_2d, y, test_size=0.20, random_state=20)\n",
    "    \n",
    "C_range = np.logspace(-5, 7, 13)\n",
    "gamma_range = np.logspace(-8, 4, 13)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "cv = KFold(n_splits=10,shuffle=True,random_state=3)\n",
    "grid = GridSearchCV(SVC(), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X_train, y_train)   \n",
    "\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                     len(gamma_range))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "           norm=MidpointNormalize(vmin=0.1,midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('K-Fold Cross Validation Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#==========================Figure 4\n",
    "plt.close('all')\n",
    "plt.figure(figsize=(8,6))\n",
    "C_2d_range = [0.1,1,10,100,1000]\n",
    "gamma_2d_range = [1e-4,1e-3, 1e-2,1e-1,1]\n",
    "classifiers = []\n",
    "for C in C_2d_range:\n",
    "    for gamma in gamma_2d_range:\n",
    "        clf = SVC(kernel='rbf', C=C, gamma=gamma)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        classifiers.append((C, gamma, clf))\n",
    "h=.1\n",
    "x_min, x_max = X_train[:, 0].min() - 2, X_train[:, 0].max() + 2\n",
    "y_min, y_max = X_train[:, 1].min() - 2, X_train[:, 1].max() + 2\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "for (k, (C, gamma, clf)) in enumerate(classifiers):\n",
    "    # evaluate decision function in a grid\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # visualize decision function for these parameters\n",
    "    plt.subplot(len(C_2d_range), len(gamma_2d_range), k + 1)\n",
    "    plt.subplots_adjust(wspace=0.07, hspace=0.27)\n",
    "    plt.title(r'$\\gamma$'\"=%g, C=%g\" % (gamma,C),\n",
    "              size='medium',weight='bold')\n",
    "\n",
    "    # visualize parameter's effect on decision function\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=pl.cm.coolwarm, alpha=0.5)\n",
    "    #plt.pcolormesh(xx, yy, -Z, cmap=plt.cm.RdBu)\n",
    "    #plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=pl.cm.coolwarm)\n",
    "    #plt.scatter(X_test[:, 0], X_test[:, 1], y_test, cmap=pl.cm.RdBu_r)\n",
    "    \n",
    "    for i in range(0, X_train.shape[0]):\n",
    "        if y_train[i] == 0:\n",
    "            c1 = pl.scatter(X_train[i,0],X_train[i,1],c='r',marker='o', s = 20 )\n",
    "        elif y_train[i] == 1:\n",
    "            c2 = pl.scatter(X_train[i,0],X_train[i,1],c='g',marker='+', s = 30 )\n",
    "    for i in range(0, X_test.shape[0]):\n",
    "        if y_test[i] == 0:\n",
    "            c3 = pl.scatter(X_test[i,0],X_test[i,1],c='b',marker='o', s = 20 )\n",
    "        elif y_test[i] == 1:\n",
    "            c4 = pl.scatter(X_test[i,0],X_test[i,1],c='m',marker='+', s = 30 )        \n",
    "    plt.text(0.7,0.8, ('%.2f'% np.mean(cross_val_score(estimator=clf, \n",
    "                         X=X_train, \n",
    "                         y=y_train, \n",
    "                         cv=KFold(n_splits=10,shuffle=True,random_state=3),\n",
    "                         n_jobs=1)) ), transform=pl.gca().transAxes,\n",
    "            bbox={'facecolor':'white'})    \n",
    "    \n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    \n",
    "\n",
    "plt.subplots_adjust(left=0.02, right=0.98, bottom=0.01, top=0.96)\n",
    "\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                     len(gamma_range))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fig 5\n",
    "# Draw heatmap of the validation accuracy as a function of gamma and C\n",
    "#\n",
    "# The score are encoded as colors with the hot colormap which varies from dark\n",
    "# red to bright yellow. As the most interesting scores are all located in the\n",
    "# 0.92 to 0.97 range we use a custom normalizer to set the mid-point to 0.92 so\n",
    "# as to make it easier to visualize the small variations of score values in the\n",
    "# interesting range while not brutally collapsing all the low score values to\n",
    "# the same color.\n",
    "\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(len(C_range),\n",
    "                                                     len(gamma_range))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplots_adjust(left=0, right=1, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot,\n",
    "           norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('K-Fold Cross Validation Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Overfit\n",
    "gamma=1\n",
    "C=10\n",
    "clf = SVC(C=C, gamma=gamma)\n",
    "clf.fit(X_train, y_train)\n",
    "h=.1\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "              size='medium')\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=pl.cm.coolwarm, alpha=0.5)\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        c1 = pl.scatter(X_train[i,0],X_train[i,1],c='r',marker='o', s = 20 )\n",
    "    elif y_train[i] == 1:\n",
    "        c2 = pl.scatter(X_train[i,0],X_train[i,1],c='g',marker='+', s = 30 )\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    if y_test[i] == 0:\n",
    "        c3 = pl.scatter(X_test[i,0],X_test[i,1],c='b',marker='o', s = 20 )\n",
    "    elif y_test[i] == 1:\n",
    "        c4 = pl.scatter(X_test[i,0],X_test[i,1],c='m',marker='+', s = 30 )     \n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Overfitting')\n",
    "pl.legend([c1, c2,c3,c4], ['Train Nnegative','Train Positive','test Nnegative','test Positive'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#good fit\n",
    "gamma=1e-2\n",
    "C=10\n",
    "clf = SVC(C=C, gamma=gamma)\n",
    "clf.fit(X_train, y_train)\n",
    "h=.1\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "              size='medium')\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=pl.cm.coolwarm, alpha=0.5)\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        c1 = pl.scatter(X_train[i,0],X_train[i,1],c='r',marker='o', s = 20 )\n",
    "    elif y_train[i] == 1:\n",
    "        c2 = pl.scatter(X_train[i,0],X_train[i,1],c='g',marker='+', s = 30 )\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    if y_test[i] == 0:\n",
    "        c3 = pl.scatter(X_test[i,0],X_test[i,1],c='b',marker='o', s = 20 )\n",
    "    elif y_test[i] == 1:\n",
    "        c4 = pl.scatter(X_test[i,0],X_test[i,1],c='m',marker='+', s = 30 )     \n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Good Fitting')\n",
    "pl.legend([c1, c2,c3,c4], ['Train Negative','Train Positive','Test Negative','Test Positive'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#underfit\n",
    "gamma=1e-4\n",
    "C=10\n",
    "clf = SVC(C=C, gamma=gamma)\n",
    "clf.fit(X_train, y_train)\n",
    "h=.1\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.title(\"gamma=10^%d, C=10^%d\" % (np.log10(gamma), np.log10(C)),\n",
    "              size='medium')\n",
    "\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=pl.cm.coolwarm, alpha=0.5)\n",
    "for i in range(0, X_train.shape[0]):\n",
    "    if y_train[i] == 0:\n",
    "        c1 = pl.scatter(X_train[i,0],X_train[i,1],c='r',marker='o', s = 20 )\n",
    "    elif y_train[i] == 1:\n",
    "        c2 = pl.scatter(X_train[i,0],X_train[i,1],c='g',marker='+', s = 30 )\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    if y_test[i] == 0:\n",
    "        c3 = pl.scatter(X_test[i,0],X_test[i,1],c='b',marker='o', s = 20 )\n",
    "    elif y_test[i] == 1:\n",
    "        c4 = pl.scatter(X_test[i,0],X_test[i,1],c='m',marker='+', s = 30 )     \n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.axis('tight')\n",
    "plt.xlabel('Under Fitting')\n",
    "pl.legend([c1, c2,c3,c4], ['Train Nnegative','Train Positive','test Nnegative','test Positive'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
